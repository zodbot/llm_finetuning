{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPdMgmMuvWtrPmiMoT3879j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zodbot/llm_finetuning/blob/main/classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/\"\n",
        "    \"LLMs-from-scratch/main/ch05/\"\n",
        "    \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "filename = url.split('/')[-1]\n",
        "urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "id": "TctkFV8Cq980"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from gpt_download import download_and_load_gpt2\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=\"124M\",\n",
        "    models_dir=\"/content/drive/MyDrive/gpt2\"\n",
        ")"
      ],
      "metadata": {
        "id": "r4r7ItkBrHxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.model import GPTModel\n",
        "from src.config import GPT_CONFIGS\n",
        "from src.utils import load_weights_into_gpt\n",
        "\n",
        "# Get configuration\n",
        "config = GPT_CONFIGS[\"gpt2-small (124M)\"]\n",
        "\n",
        "# Set up model for classification\n",
        "model = GPTModel(config)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval()\n",
        "\n",
        "# Freeze model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify for classification\n",
        "num_classes = 2\n",
        "model.out_head = torch.nn.Linear(config[\"emb_dim\"], num_classes)\n",
        "\n",
        "# Unfreeze specific layers\n",
        "for param in model.trf_blocks[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model.final_norm.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "obkdr1nOrNta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEW1-BkhevDB"
      },
      "outputs": [],
      "source": [
        "# SST-2 (Stanford Sentiment Treebank) is a great dataset for binary sentiment classification\n",
        "from datasets import load_dataset\n",
        "ds = load_dataset('nyu-mll/glue', 'sst2', split='train')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to pandas DataFrame\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "    'Label': ds['label'],\n",
        "    'Text': ds['sentence']\n",
        "})\n",
        "\n",
        "print(len(df))\n",
        "print(df[\"Label\"].value_counts())\n",
        "\n",
        "# Take random sample of 1000 rows\n",
        "df_sample = df.sample(n=10000, random_state=42)  # random_state for reproducibility\n",
        "\n",
        "# Optional: Look at the distribution of labels to ensure it's balanced\n",
        "print(\"Label distribution in sample:\")\n",
        "print(df_sample['Label'].value_counts())\n",
        "\n",
        "def random_split(df, train_frac, validation_frac):\n",
        "    df = df.sample(\n",
        "        frac=1, random_state=123\n",
        "    ).reset_index(drop=True)\n",
        "    train_end = int(len(df) * train_frac)\n",
        "    validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "    train_df = df[:train_end]\n",
        "    validation_df = df[train_end:validation_end]\n",
        "    test_df = df[validation_end:]\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "train_df, validation_df, test_df = random_split(df, 0.7, 0.1)\n",
        "\n",
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)\n",
        "\n",
        "f = open(\"train.csv\")\n",
        "inputs = []\n",
        "max_length = 0\n",
        "for line in f.read():\n",
        "  ids = tokenizer.encode(line)\n",
        "  max_length = max(max_length, len(ids))\n",
        "  inputs.append(ids)\n",
        "\n",
        "\n",
        "for input in inputs:\n",
        "  for _ in range(max_length - len(input)):\n",
        "    input.append(5027)\n",
        "\n"
      ],
      "metadata": {
        "id": "lb-7Ox82fNJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "# it identifies the longest sequence in the training dataset, encodes the text messages,\n",
        "# and ensures that all other sequences are padded with a padding token to match the length of the longest sequence.\n",
        "class Sst2DataSet(Dataset):\n",
        "  def __init__(self, csv_file, tokenizer, max_length=None,\n",
        "                 pad_token_id=50256):\n",
        "      self.data = pd.read_csv(csv_file)\n",
        "      self.encoded_texts = [tokenizer.encode(data) for data in self.data[\"Text\"]]\n",
        "      if max_length is None:\n",
        "          self.max_length = self._longest_length()\n",
        "      else:\n",
        "          self.max_length = max_length\n",
        "      # Truncates sequences if they are longer than max_length\n",
        "      self.encoded_texts = [\n",
        "                  encoded_text[:self.max_length]\n",
        "                  for encoded_text in self.encoded_texts\n",
        "      ]\n",
        "      # add padding\n",
        "      self.encoded_texts = [\n",
        "        encoded_text + [pad_token_id] *\n",
        "        (self.max_length - len(encoded_text))\n",
        "        for encoded_text in self.encoded_texts\n",
        "      ]\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      encoded = self.encoded_texts[index]\n",
        "      label = self.data.iloc[index][\"Label\"]\n",
        "      return (\n",
        "          torch.tensor(encoded, dtype=torch.long),\n",
        "          torch.tensor(label, dtype=torch.long)\n",
        "      )\n",
        "  def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "  def _longest_length(self):\n",
        "      max_length = 0\n",
        "      for encoded_text in self.encoded_texts:\n",
        "          encoded_length = len(encoded_text)\n",
        "          if encoded_length > max_length:\n",
        "              max_length = encoded_length\n",
        "      return max_length\n",
        "\n",
        "\n",
        "train_dataset = Sst2DataSet(\n",
        "    csv_file=\"train.csv\",\n",
        "    max_length=None,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "val_dataset = Sst2DataSet(\n",
        "    csv_file=\"validation.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "test_dataset = Sst2DataSet(\n",
        "    csv_file=\"test.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "print(train_dataset.max_length)"
      ],
      "metadata": {
        "id": "qvCDN73OfTjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) change the last layer and fine-tune just the final transformer block\n",
        "\n",
        "import sys\n",
        "\n",
        "\n",
        "!git clone https://github.com/zodbot/llm_finetuning.git\n",
        "\n",
        "# Change into repo directory\n",
        "%cd llm_finetuning\n",
        "\n",
        "model = GPTModel(config)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval()\n",
        "\n",
        "# freeze the model\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# add a classification layer\n",
        "num_classes = 2\n",
        "model.out_head = torch.nn.Linear(config[\"emb_dim\"], num_classes)\n",
        "\n",
        "for param in model.trf_blocks[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model.final_norm.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "-gLwGKHffZMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tx4z-vGRpiUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(data_loader, model, device, num_batches=None):\n",
        "    model.eval()\n",
        "    correct_predictions, num_examples = 0, 0\n",
        "\n",
        "    if not num_batches:\n",
        "      return\n",
        "    model.eval()\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "      if i < num_batches:\n",
        "        input_batch = input_batch.to(device)\n",
        "        target_batch = target_batch.to(device)\n",
        "        with torch.no_grad():\n",
        "          output = model(input_batch)\n",
        "        output = output[:, -1, :]\n",
        "        result = torch.argmax(output, -1)\n",
        "        correct_predictions += sum(predicted == target_batch[i] for i, predicted in enumerate(result))\n",
        "        num_examples += output.shape[0]\n",
        "\n",
        "    return correct_predictions / num_examples\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "torch.manual_seed(123)\n",
        "train_accuracy = calc_accuracy(\n",
        "    train_loader, model, device, num_batches=10\n",
        ")\n",
        "val_accuracy = calc_accuracy(\n",
        "    val_loader, model, device, num_batches=10\n",
        ")\n",
        "test_accuracy = calc_accuracy(\n",
        "    test_loader, model, device, num_batches=10\n",
        ")\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "bBe7nwmbfcsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss(data_loader, model, device, num_batches=None):\n",
        "  if len(data_loader) == 0:\n",
        "    return float(\"nan\")\n",
        "\n",
        "  if num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  all_loss = 0\n",
        "\n",
        "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "    if i < num_batches:\n",
        "      input_batch = input_batch.to(device)\n",
        "      target_batch = target_batch.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        predicted = model(input_batch)\n",
        "      # we only care about the last token\n",
        "      predicted = predicted[:, -1, :]\n",
        "      loss = torch.nn.functional.cross_entropy(predicted, target_batch)\n",
        "      all_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  return all_loss / num_batches"
      ],
      "metadata": {
        "id": "tsWLXV1Mffeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "  input_batch = input_batch.to(device)\n",
        "  target_batch = target_batch.to(device)\n",
        "  logits = model(input_batch)[:, -1, :]\n",
        "  loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
        "  return loss\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      train_loss = calc_loss(\n",
        "            train_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "      val_loss = calc_loss(\n",
        "            val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "def train_classifier(model, data_loader, val_loader, optimizer, device,\n",
        "               epoch_num, eval_freq, eval_iter):\n",
        "  model.train()\n",
        "  examples_seen = 0\n",
        "  steps = -1\n",
        "  train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "\n",
        "  for epoch in range(epoch_num):\n",
        "    model.train()\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "      # reset optimizer\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "\n",
        "      # cal loss gradient\n",
        "      loss.backward()\n",
        "\n",
        "      # update weight\n",
        "      optimizer.step()\n",
        "      # we care about number of smples not tokens here\n",
        "      examples_seen += input_batch.shape[0]\n",
        "      steps += 1\n",
        "\n",
        "      if steps % eval_freq == 0:\n",
        "          train_loss, val_loss = evaluate_model(\n",
        "            model, data_loader, val_loader, device, eval_iter)\n",
        "          train_losses.append(train_loss)\n",
        "          val_losses.append(val_loss)\n",
        "          print(f\"Ep {epoch+1} (Step {steps:06d}): \"\n",
        "                f\"Train loss {train_loss:.3f}, \"\n",
        "                f\"Val loss {val_loss:.3f}\")\n",
        "\n",
        "    train_accuracy = calc_accuracy(\n",
        "      data_loader, model, device, num_batches=eval_iter\n",
        "    )\n",
        "    val_accuracy = calc_accuracy(\n",
        "      val_loader, model, device, num_batches=eval_iter\n",
        "    )\n",
        "    print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "    print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "    train_accs.append(train_accuracy)\n",
        "    val_accs.append(val_accuracy)\n",
        "\n",
        "  return train_losses, val_losses, train_accs, val_accs, examples_seen\n"
      ],
      "metadata": {
        "id": "bOs6BNgnfj0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.1)\n",
        "num_epochs = 5\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = \\\n",
        "    train_classifier(\n",
        "        model, train_loader, val_loader, optimizer, device,\n",
        "        num_epochs, eval_freq=2000,\n",
        "        eval_iter=5\n",
        ")\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "id": "8e-kT2R1foKK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}