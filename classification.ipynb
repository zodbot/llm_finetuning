{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNZFNr8dbxQ/8ucP/vWVaMY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zodbot/llm_finetuning/blob/main/classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating Fine-tuned GPT-2 Sentiment Classifier\n",
        "\n",
        "This notebook:\n",
        "1. Loads previously fine-tuned GPT-2 model\n",
        "2. Evaluates performance on test set\n",
        "3. Provides function for classifying new reviews\n",
        "4. Demonstrates usage with example reviews\n",
        "\n",
        "Model details:\n",
        "- Base: GPT-2 fine-tuned on SST-2\n",
        "- Task: Binary sentiment classification\n",
        "- Output: Positive/Negative with confidence scores"
      ],
      "metadata": {
        "id": "BuqHeUQQk2vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "import urllib.request\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/\"\n",
        "    \"LLMs-from-scratch/main/ch05/\"\n",
        "    \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "filename = url.split('/')[-1]\n",
        "urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "id": "TctkFV8Cq980",
        "outputId": "8efde14f-9ca3-42fb-ef2a-2e28b7f0b92a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt_download.py', <http.client.HTTPMessage at 0x7f8f7d936d90>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from gpt_download import download_and_load_gpt2\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=\"124M\",\n",
        "    models_dir=\"/content/drive/MyDrive/gpt2\"\n",
        ")"
      ],
      "metadata": {
        "id": "r4r7ItkBrHxr",
        "outputId": "fa6c7ac5-e758-4861-9fbc-92ced9bb8b16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "File already exists and is up-to-date: /content/drive/MyDrive/gpt2/124M/checkpoint\n",
            "File already exists and is up-to-date: /content/drive/MyDrive/gpt2/124M/encoder.json\n",
            "File already exists and is up-to-date: /content/drive/MyDrive/gpt2/124M/hparams.json\n",
            "File already exists and is up-to-date: /content/drive/MyDrive/gpt2/124M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: /content/drive/MyDrive/gpt2/124M/model.ckpt.index\n",
            "File already exists and is up-to-date: /content/drive/MyDrive/gpt2/124M/model.ckpt.meta\n",
            "File already exists and is up-to-date: /content/drive/MyDrive/gpt2/124M/vocab.bpe\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a5987e785335>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgpt_download\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_and_load_gpt2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m settings, params = download_and_load_gpt2(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"124M\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodels_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/gpt2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gpt_download.py\u001b[0m in \u001b[0;36mdownload_and_load_gpt2\u001b[0;34m(model_size, models_dir)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mtf_ckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0msettings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hparams.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_gpt2_params_from_tf_ckpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_ckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gpt_download.py\u001b[0m in \u001b[0;36mload_gpt2_params_from_tf_ckpt\u001b[0;34m(ckpt_path, settings)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Load the variable and remove singleton dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mvariable_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Process the variable name to extract relevant parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/training/checkpoint_utils.py\u001b[0m in \u001b[0;36mload_variable\u001b[0;34m(ckpt_dir_or_file, name)\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m   \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_dir_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/training/checkpoint_utils.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(ckpt_dir_or_file)\u001b[0m\n\u001b[1;32m     78\u001b[0m     raise ValueError(\"Couldn't find 'checkpoint' file or checkpoints in \"\n\u001b[1;32m     79\u001b[0m                      \"given directory %s\" % ckpt_dir_or_file)\n\u001b[0;32m---> 80\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mpy_checkpoint_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     90\u001b[0m   \"\"\"\n\u001b[1;32m     91\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m   \u001b[0;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "\n",
        "!git clone https://github.com/zodbot/llm_finetuning.git\n",
        "\n",
        "# Change into repo directory\n",
        "%cd llm_finetuning\n",
        "\n",
        "from src.model import GPTModel\n",
        "from src.config import GPT_CONFIGS\n",
        "from src.utils import load_weights_into_gpt\n",
        "import torch\n",
        "\n",
        "# Get configuration\n",
        "config = GPT_CONFIGS[\"gpt2-small (124M)\"]\n",
        "\n",
        "# Set up model for classification\n",
        "model = GPTModel(config)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval()\n",
        "\n",
        "# Freeze model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify for classification\n",
        "num_classes = 2\n",
        "model.out_head = torch.nn.Linear(config[\"emb_dim\"], num_classes)\n",
        "# model.out_head = torch.nn.Sequential(\n",
        "#     torch.nn.Dropout(0.3),\n",
        "#     torch.nn.Linear(config[\"emb_dim\"], num_classes)\n",
        "# )\n",
        "# Unfreeze specific layers\n",
        "for param in model.trf_blocks[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model.final_norm.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "obkdr1nOrNta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eOXkWMPCrl4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEW1-BkhevDB"
      },
      "outputs": [],
      "source": [
        "# SST-2 (Stanford Sentiment Treebank) is a great dataset for binary sentiment classification\n",
        "from datasets import load_dataset\n",
        "ds = load_dataset('nyu-mll/glue', 'sst2', split='train')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "4u3a06TUr0VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to pandas DataFrame\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Label': ds['label'],\n",
        "    'Text': ds['sentence']\n",
        "})\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "print(len(df))\n",
        "print(df[\"Label\"].value_counts())\n",
        "\n",
        "# Take random sample of 1000 rows\n",
        "df_sample = df.sample(n=5000, random_state=42)  # random_state for reproducibility\n",
        "\n",
        "# Optional: Look at the distribution of labels to ensure it's balanced\n",
        "print(\"Label distribution in sample:\")\n",
        "print(df_sample['Label'].value_counts())\n",
        "\n",
        "def random_split(df, train_frac, validation_frac):\n",
        "    df = df.sample(\n",
        "        frac=1, random_state=123\n",
        "    ).reset_index(drop=True)\n",
        "    train_end = int(len(df) * train_frac)\n",
        "    validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "    train_df = df[:train_end]\n",
        "    validation_df = df[train_end:validation_end]\n",
        "    test_df = df[validation_end:]\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "train_df, validation_df, test_df = random_split(df, 0.7, 0.1)\n",
        "\n",
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)\n",
        "\n",
        "f = open(\"train.csv\")\n",
        "inputs = []\n",
        "max_length = 0\n",
        "for line in f.read():\n",
        "  ids = tokenizer.encode(line)\n",
        "  max_length = max(max_length, len(ids))\n",
        "  inputs.append(ids)\n",
        "\n",
        "\n",
        "for input in inputs:\n",
        "  for _ in range(max_length - len(input)):\n",
        "    input.append(5027)\n",
        "\n"
      ],
      "metadata": {
        "id": "lb-7Ox82fNJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "# it identifies the longest sequence in the training dataset, encodes the text messages,\n",
        "# and ensures that all other sequences are padded with a padding token to match the length of the longest sequence.\n",
        "class Sst2DataSet(Dataset):\n",
        "  def __init__(self, csv_file, tokenizer, max_length=None,\n",
        "                 pad_token_id=50256):\n",
        "      self.data = pd.read_csv(csv_file)\n",
        "      self.encoded_texts = [tokenizer.encode(data) for data in self.data[\"Text\"]]\n",
        "      if max_length is None:\n",
        "          self.max_length = self._longest_length()\n",
        "      else:\n",
        "          self.max_length = max_length\n",
        "      # Truncates sequences if they are longer than max_length\n",
        "      self.encoded_texts = [\n",
        "                  encoded_text[:self.max_length]\n",
        "                  for encoded_text in self.encoded_texts\n",
        "      ]\n",
        "      # add padding\n",
        "      self.encoded_texts = [\n",
        "        encoded_text + [pad_token_id] *\n",
        "        (self.max_length - len(encoded_text))\n",
        "        for encoded_text in self.encoded_texts\n",
        "      ]\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      encoded = self.encoded_texts[index]\n",
        "      label = self.data.iloc[index][\"Label\"]\n",
        "      return (\n",
        "          torch.tensor(encoded, dtype=torch.long),\n",
        "          torch.tensor(label, dtype=torch.long)\n",
        "      )\n",
        "  def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "  def _longest_length(self):\n",
        "      max_length = 0\n",
        "      for encoded_text in self.encoded_texts:\n",
        "          encoded_length = len(encoded_text)\n",
        "          if encoded_length > max_length:\n",
        "              max_length = encoded_length\n",
        "      return max_length\n",
        "\n",
        "\n",
        "train_dataset = Sst2DataSet(\n",
        "    csv_file=\"train.csv\",\n",
        "    max_length=None,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "val_dataset = Sst2DataSet(\n",
        "    csv_file=\"validation.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "test_dataset = Sst2DataSet(\n",
        "    csv_file=\"test.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "print(train_dataset.max_length)"
      ],
      "metadata": {
        "id": "qvCDN73OfTjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "torch.manual_seed(123)\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "print(\"# of batches: \", len(train_loader))\n",
        "for train, target in train_loader:\n",
        "  print(train.shape, target.shape)\n",
        "  break\n"
      ],
      "metadata": {
        "id": "Rgz_jOvUsaLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tx4z-vGRpiUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "  input_batch = input_batch.to(device)\n",
        "  target_batch = target_batch.to(device)\n",
        "  logits = model(input_batch)[:, -1, :]\n",
        "  loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
        "  return loss\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      train_loss = calc_loss(\n",
        "            train_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "      val_loss = calc_loss(\n",
        "            val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "def train_classifier(model, data_loader, val_loader, optimizer, device,\n",
        "               epoch_num, eval_freq, eval_iter):\n",
        "  model.train()\n",
        "  examples_seen = 0\n",
        "  steps = -1\n",
        "  train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "\n",
        "  for epoch in range(epoch_num):\n",
        "    model.train()\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "      input_batch = input_batch.to(device)\n",
        "      target_batch = target_batch.to(device)\n",
        "      # reset optimizer\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "\n",
        "      # cal loss gradient\n",
        "      loss.backward()\n",
        "\n",
        "      # update weight\n",
        "      optimizer.step()\n",
        "      # we care about number of smples not tokens here\n",
        "      examples_seen += input_batch.shape[0]\n",
        "      steps += 1\n",
        "\n",
        "      if steps % eval_freq == 0:\n",
        "          train_loss, val_loss = evaluate_model(\n",
        "            model, data_loader, val_loader, device, eval_iter)\n",
        "          train_losses.append(train_loss)\n",
        "          val_losses.append(val_loss)\n",
        "          print(f\"Ep {epoch+1} (Step {steps:06d}): \"\n",
        "                f\"Train loss {train_loss:.3f}, \"\n",
        "                f\"Val loss {val_loss:.3f}\")\n",
        "\n",
        "    train_accuracy = calc_accuracy(\n",
        "      data_loader, model, device, num_batches=eval_iter\n",
        "    )\n",
        "    val_accuracy = calc_accuracy(\n",
        "      val_loader, model, device, num_batches=eval_iter\n",
        "    )\n",
        "    print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "    print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "    train_accs.append(train_accuracy)\n",
        "    val_accs.append(val_accuracy)\n",
        "\n",
        "  return train_losses, val_losses, train_accs, val_accs, examples_seen\n"
      ],
      "metadata": {
        "id": "bOs6BNgnfj0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.1)\n",
        "num_epochs = 5\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = \\\n",
        "    train_classifier(\n",
        "        model, train_loader, val_loader, optimizer, device,\n",
        "        num_epochs, eval_freq=2000,\n",
        "        eval_iter=5\n",
        ")\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "id": "8e-kT2R1foKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# changed the weight decay\n",
        "import time\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "num_epochs = 5\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = \\\n",
        "    train_classifier(\n",
        "        model, train_loader, val_loader, optimizer, device,\n",
        "        num_epochs, eval_freq=2000,\n",
        "        eval_iter=5\n",
        ")\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "hS2T_1nf0xLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'config': config,\n",
        "    'unfrozen_layers': ['trf_blocks[-1]', 'final_norm']  # document which layers were unfrozen\n",
        "}, '/content/drive/MyDrive/gpt2_finetuned_sst.pt')"
      ],
      "metadata": {
        "id": "an6MHrO5-VEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_values(\n",
        "        epochs_seen, examples_seen, train_values, val_values,\n",
        "  label=\"loss\"):\n",
        "  fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "  ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
        "  ax1.plot(\n",
        "  epochs_seen, val_values, linestyle=\"-.\",\n",
        "      label=f\"Validation {label}\"\n",
        "  )\n",
        "  ax1.set_xlabel(\"Epochs\")\n",
        "  ax1.set_ylabel(label.capitalize())\n",
        "  ax1.legend()\n",
        "\n",
        "  ax2 = ax1.twiny()\n",
        "  ax2.plot(examples_seen, train_values, alpha=0)\n",
        "  ax2.set_xlabel(\"Examples seen\")\n",
        "\n",
        "  fig.tight_layout()\n",
        "  plt.savefig(f\"{label}-plot.pdf\")\n",
        "  plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
      ],
      "metadata": {
        "id": "BI9gRQCH8Cqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "loaded_state = torch.load('/content/drive/MyDrive/gpt2_finetuned_sst.pt',\n",
        "                         map_location=device)\n",
        "\n",
        "!git clone https://github.com/zodbot/llm_finetuning.git\n",
        "\n",
        "# Change into repo directory\n",
        "%cd llm_finetuning\n",
        "\n",
        "from src.model import GPTModel\n",
        "from src.config import GPT_CONFIGS\n",
        "from src.utils import load_weights_into_gpt\n",
        "import torch\n",
        "\n",
        "# Get configuration\n",
        "config = GPT_CONFIGS[\"gpt2-small (124M)\"]\n",
        "\n",
        "model = GPTModel(config)\n",
        "\n",
        "# 2. Modify model architecture for classification (just like during training)\n",
        "num_classes = 2\n",
        "model.out_head = torch.nn.Linear(config[\"emb_dim\"], num_classes)\n",
        "\n",
        "# load the state dict\n",
        "model.load_state_dict(loaded_state['model_state_dict'])\n"
      ],
      "metadata": {
        "id": "9-XUieByGqmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(data_loader, model, device, num_batches=None):\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "\n",
        "            input_batch = input_batch.to(device)\n",
        "            target_batch = target_batch.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_batch)\n",
        "            outputs = outputs[:, -1, :]  # Get last token predictions\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            total += target_batch.size(0)\n",
        "            correct += (predicted == target_batch).sum().item()\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    return accuracy  # Make sure we return the accuracy!\n",
        "\n",
        "train_accuracy = calc_accuracy(train_loader, model, device, 4)\n",
        "val_accuracy = calc_accuracy(val_loader, model, device, 4)\n",
        "test_accuracy = calc_accuracy(test_loader, model, device, 4)\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "lH1S6yIcHhun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_token_ids(text, tokenizer, max_length=None, pad_token_id=50256):\n",
        "    # Encode text to token IDs\n",
        "    tokens = tokenizer.encode(text)\n",
        "\n",
        "    # Convert to tensor and add batch dimension\n",
        "    tokens = torch.tensor(tokens).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Handle max length if specified\n",
        "    if max_length is not None:\n",
        "        if tokens.size(1) > max_length:\n",
        "            # Truncate if too long\n",
        "            tokens = tokens[:, :max_length]\n",
        "        elif tokens.size(1) < max_length:\n",
        "            # Pad if too short\n",
        "            padding = torch.full((1, max_length - tokens.size(1)),\n",
        "                               pad_token_id,\n",
        "                               dtype=torch.long)\n",
        "            tokens = torch.cat([tokens, padding], dim=1)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Example usage:\n",
        "# text = \"This is a great movie!\"\n",
        "# token_ids = text_to_token_ids(text, tokenizer, max_length=512)\n",
        "# print(token_ids.shape)  # Should be [1, sequence_length]\n",
        "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Tokenize the input text\n",
        "        tokens = text_to_token_ids(text, tokenizer).to(device)\n",
        "\n",
        "        # Get model prediction\n",
        "        logits = model(tokens)\n",
        "        logits = logits[:, -1, :]  # Get last token's logits\n",
        "\n",
        "        # Convert to probabilities\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        prediction = torch.argmax(probs, dim=-1).item()\n",
        "        confidence = probs[0, prediction].item()\n",
        "\n",
        "        # Convert to sentiment\n",
        "        sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
        "\n",
        "        return {\n",
        "            \"sentiment\": sentiment,\n",
        "            \"confidence\": confidence,\n",
        "            \"probabilities\": {\n",
        "                \"negative\": probs[0, 0].item(),\n",
        "                \"positive\": probs[0, 1].item()\n",
        "            }\n",
        "        }\n",
        "\n",
        "# Example usage:\n",
        "test_reviews = [\n",
        "    \"This movie was absolutely fantastic!\",\n",
        "    \"What a terrible waste of time.\",\n",
        "    \"The acting was okay, but the plot was confusing.\"\n",
        "]\n",
        "\n",
        "for review in test_reviews:\n",
        "    result = classify_review(review, model, tokenizer, device)\n",
        "    print(f\"\\nReview: {review}\")\n",
        "    print(f\"Sentiment: {result['sentiment']}\")\n",
        "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
        "    print(f\"Probabilities: Positive: {result['probabilities']['positive']:.2%}, \"\n",
        "          f\"Negative: {result['probabilities']['negative']:.2%}\")"
      ],
      "metadata": {
        "id": "1c2y-Q1fIim2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fGFnPW6ZkQ_u"
      }
    }
  ]
}