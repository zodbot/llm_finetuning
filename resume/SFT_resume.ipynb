{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP7cnZvsbgzs/IF1PDt46jC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f97ff4a9445f470c86690bed37b922b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_692c897a2adb476b9dc7acc095512a19",
              "IPY_MODEL_018601b6ef154dfc9fd567a7a41c2f9f",
              "IPY_MODEL_2882bd18faf34f4ea97a48db2e6be59f"
            ],
            "layout": "IPY_MODEL_abee715a09bb4a7984d01f6d84f91ee1"
          }
        },
        "692c897a2adb476b9dc7acc095512a19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adf18374dd8f46e9ad6053966e8e4e2b",
            "placeholder": "​",
            "style": "IPY_MODEL_b435a352893b482084e3bcdd1c71f6c7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "018601b6ef154dfc9fd567a7a41c2f9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b52dc6cc72d7449fbb9234fca27a2a24",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63b588fd7fe64fedac9d851f5fe4f6e2",
            "value": 2
          }
        },
        "2882bd18faf34f4ea97a48db2e6be59f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29a09a4d30804df2a9cdb767a11eb4ee",
            "placeholder": "​",
            "style": "IPY_MODEL_9625be00d2364a7e8a6db4d4fe7f1b04",
            "value": " 2/2 [00:07&lt;00:00,  3.62s/it]"
          }
        },
        "abee715a09bb4a7984d01f6d84f91ee1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adf18374dd8f46e9ad6053966e8e4e2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b435a352893b482084e3bcdd1c71f6c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b52dc6cc72d7449fbb9234fca27a2a24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63b588fd7fe64fedac9d851f5fe4f6e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29a09a4d30804df2a9cdb767a11eb4ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9625be00d2364a7e8a6db4d4fe7f1b04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zodbot/llm_finetuning/blob/main/resume/SFT_resume.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers>=4.34.0\n",
        "!pip install trl>=0.7.2"
      ],
      "metadata": {
        "id": "6oRoIgo2LoRm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instruction Fine-tuning with TinyLlama\n",
        "\n",
        "!pip install torch numpy pandas transformers datasets peft trl accelerate bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xanfPMEsIqFC",
        "outputId": "ae264e07-a769-45d0-c307-e39db059525d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.0)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.21.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.47.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "RuESx2hKIIIY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Mount Google Drive for saving/loading data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the path where you want to save/load data\n",
        "BASE_PATH = \"/content/drive/MyDrive/resume_skill_scoring\"\n",
        "os.makedirs(BASE_PATH, exist_ok=True)\n",
        "\n",
        "# Format dataset with XML tags for instruction tuning\n",
        "def format_with_tags(dataset):\n",
        "    \"\"\"Format data for instruction-based fine-tuning using clear XML tags.\"\"\"\n",
        "    formatted_data = []\n",
        "    for _, entry in dataset.iterrows():\n",
        "        text = f\"\"\"<instruction>Evaluate the level of expertise for a specific skill in a resume.</instruction>\n",
        "\n",
        "<resume>\n",
        "{entry['resume']}\n",
        "</resume>\n",
        "\n",
        "<skill>{entry['skill_evaluated']}</skill>\n",
        "\n",
        "<rating_scale>\n",
        "0: Not mentioned\n",
        "1: Mentioned but no evidence of usage\n",
        "2: Basic usage demonstrated\n",
        "3: Moderate competency shown\n",
        "4: Strong competency with specific achievements\n",
        "5: Expert level with leadership/teaching in that skill\n",
        "</rating_scale>\n",
        "\n",
        "<answer>\n",
        "<rating>{entry['score']}</rating>\n",
        "<reasoning>{entry['reasoning']}</reasoning>\n",
        "<evidence>{entry['evidence']}</evidence>\n",
        "</answer>\"\"\"\n",
        "        formatted_data.append({\n",
        "            \"text\": text\n",
        "        })\n",
        "    return formatted_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1ogtMnCILZL",
        "outputId": "a6b888da-2554-4fcd-a0be-992cb6abdb62"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load or generate dataset\n",
        "def load_or_generate_dataset():\n",
        "    \"\"\"Load dataset from JSON files or generate if needed.\"\"\"\n",
        "    input_dir = '/content/drive/MyDrive/resume_data'\n",
        "    all_data = []\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if filename.endswith('.json'):\n",
        "            file_path = os.path.join(input_dir, filename)\n",
        "            try:\n",
        "                data = pd.read_json(file_path)\n",
        "                all_data.append(data)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {filename}: {e}\")\n",
        "    if all_data:\n",
        "        return pd.concat(all_data, ignore_index=True)\n",
        "    else:\n",
        "        raise ValueError(\"No valid JSON files found in the directory.\")"
      ],
      "metadata": {
        "id": "WLdOpkVeIRtp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into train/validation/test\n",
        "def split_dataset(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
        "    \"\"\"Split the dataset into train, validation and test sets.\"\"\"\n",
        "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-10\n",
        "\n",
        "    # Convert data to list if it's not already\n",
        "    data_list = data\n",
        "\n",
        "    # Shuffle data indices\n",
        "    indices = np.arange(len(data_list))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    # Calculate split indices\n",
        "    train_end = int(len(data_list) * train_ratio)\n",
        "    val_end = train_end + int(len(data_list) * val_ratio)\n",
        "\n",
        "    # Split data\n",
        "    train_data = [data_list[i] for i in indices[:train_end]]\n",
        "    val_data = [data_list[i] for i in indices[train_end:val_end]]\n",
        "    test_data = [data_list[i] for i in indices[val_end:]]\n",
        "\n",
        "    print(f\"Dataset split: {len(train_data)} training, {len(val_data)} validation, {len(test_data)} test samples\")\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "# Create HF datasets\n",
        "def create_hf_datasets(train_data, val_data, format_type=\"text\"):\n",
        "    \"\"\"Convert to HuggingFace datasets format.\"\"\"\n",
        "    train_dataset = Dataset.from_list(train_data)\n",
        "    val_dataset = Dataset.from_list(val_data)\n",
        "    return train_dataset, val_dataset"
      ],
      "metadata": {
        "id": "y5kxPhxLIVU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up and fine-tune the model\n",
        "def setup_and_train_model(train_dataset, val_dataset, format_type=\"text\",\n",
        "                          # model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "                          model_name=\"microsoft/Phi-3-medium-128k-instruct\",\n",
        "                          output_dir=None):\n",
        "    \"\"\"Set up and fine-tune the model.\"\"\"\n",
        "    if output_dir is None:\n",
        "        output_dir = f\"{BASE_PATH}/fine-tuned-model_Phi-3_medium\"\n",
        "\n",
        "    # Define quantization config for lower memory usage\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    # Load model with quantization\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Prepare model for k-bit training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # LoRA configuration\n",
        "    peft_config = LoraConfig(\n",
        "        r=16,  # Rank\n",
        "        lora_alpha=32,  # Alpha parameter for LoRA scaling\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Target attention modules\n",
        "        lora_dropout=0.05,  # Dropout probability for LoRA layers\n",
        "        bias=\"none\",  # Don't train bias\n",
        "        task_type=\"CAUSAL_LM\"  # Task type\n",
        "    )\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        gradient_checkpointing=True,\n",
        "        optim=\"adamw_torch\",\n",
        "        learning_rate=2e-4,\n",
        "        weight_decay=0.01,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        eval_steps=50,\n",
        "        save_steps=100,\n",
        "        warmup_steps=10,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    # Set up SFT trainer with appropriate dataset field\n",
        "    # Changed parameter from dataset_text_field to input_field_name to match trl 0.16.1\n",
        "    input_field_name = \"text\" if format_type == \"text\" else \"instruction\"\n",
        "\n",
        "   # Set up SFT trainer with appropriate dataset field\n",
        "    dataset_text_field = \"text\" if format_type == \"text\" else \"instruction\"\n",
        "    def formatting_func(examples):\n",
        "      if format_type == \"text\":\n",
        "          # Return the string directly, not a dictionary\n",
        "          return examples[\"text\"]\n",
        "      else:\n",
        "          # Return the combined string directly, not a dictionary\n",
        "          return examples[\"instruction\"] + examples[\"output\"]\n",
        "\n",
        "    tokenizer.model_max_length = 1024\n",
        "\n",
        "    # Then initialize SFTTrainer without max_seq_length parameter\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        peft_config=peft_config,\n",
        "        formatting_func=formatting_func,\n",
        "        processing_class=tokenizer,\n",
        "    )\n",
        "\n",
        "    # Print trainable parameters\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}% of total)\")\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    print(f\"Saving model to {output_dir}\")\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    return output_dir"
      ],
      "metadata": {
        "id": "e8sVjuXPIaUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to test the model\n",
        "def test_model(model_path, test_data, format_type=\"tagged\", num_samples=5):\n",
        "    \"\"\"Test the fine-tuned model on a few examples.\"\"\"\n",
        "\n",
        "    # Define the quantization config\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    # Method 1: Use AutoPeftModelForCausalLM for simpler loading\n",
        "    try:\n",
        "        from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "        # Load the model with the proper config\n",
        "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        print(\"Loaded model using AutoPeftModelForCausalLM\")\n",
        "    except Exception as e:\n",
        "        print(f\"Couldn't load with AutoPeftModelForCausalLM: {e}\")\n",
        "        print(\"Falling back to manual loading...\")\n",
        "\n",
        "        # Method 2: Load base model and adapter separately\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        from peft import PeftModel\n",
        "        model = PeftModel.from_pretrained(\n",
        "            base_model,\n",
        "            model_path  # Path to your fine-tuned adapters\n",
        "        )\n",
        "        print(\"Loaded model using PeftModel\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    print(\"Loaded tokenizer\")\n",
        "\n",
        "    # Select random samples\n",
        "    if num_samples > len(test_data):\n",
        "        num_samples = len(test_data)\n",
        "\n",
        "    sample_indices = np.random.choice(len(test_data), num_samples, replace=False)\n",
        "    samples = [test_data[i] for i in sample_indices]\n",
        "\n",
        "    # Test each sample\n",
        "    results = []\n",
        "    for i, sample in enumerate(samples):\n",
        "        print(f\"\\nTesting sample {i+1}/{num_samples}\")\n",
        "\n",
        "        # Process input based on format type\n",
        "        if format_type == \"text\":\n",
        "            # Legacy format with <output> tag\n",
        "            input_text = sample[\"text\"].split(\"<output>\")[0]\n",
        "            expected = sample[\"text\"].split(\"<output>\")[1].strip()\n",
        "        elif format_type == \"tagged\":\n",
        "            # New tagged format with <answer> tag\n",
        "            input_text = sample[\"text\"].split(\"<answer>\")[0] + \"<answer>\"\n",
        "            expected = sample[\"text\"].split(\"<answer>\")[1].strip()\n",
        "        else:\n",
        "            # Default instruction/output format\n",
        "            input_text = sample[\"instruction\"]\n",
        "            expected = sample[\"output\"]\n",
        "\n",
        "        # Generate prediction with improved parameters\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        try:\n",
        "            outputs = model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_new_tokens=200,\n",
        "                temperature=0.1,\n",
        "                do_sample=False,\n",
        "                use_cache=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "            # Decode prediction and extract just the generated part\n",
        "            full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            prediction = full_text[len(input_text):].strip()\n",
        "\n",
        "            # Print results (shortened for readability)\n",
        "            print(f\"Input: {input_text[:100]}...\")\n",
        "            print(f\"Expected: {expected[:100]}...\" if len(expected) > 100 else f\"Expected: {expected}\")\n",
        "            print(f\"Prediction: {prediction[:100]}...\" if len(prediction) > 100 else f\"Prediction: {prediction}\")\n",
        "\n",
        "            # Try to extract score using multiple methods\n",
        "            try:\n",
        "                # Method 1: Look for <rating> tags\n",
        "                if \"<rating>\" in prediction and \"</rating>\" in prediction:\n",
        "                    import re\n",
        "                    rating_match = re.search(r'<rating>(.*?)</rating>', prediction, re.DOTALL)\n",
        "                    pred_score_text = rating_match.group(1).strip() if rating_match else None\n",
        "\n",
        "                    rating_match = re.search(r'<rating>(.*?)</rating>', expected, re.DOTALL)\n",
        "                    expected_score_text = rating_match.group(1).strip() if rating_match else None\n",
        "\n",
        "                # Method 2: Look for <score> tags (legacy format)\n",
        "                elif \"<score>\" in prediction and \"</score>\" in prediction:\n",
        "                    import re\n",
        "                    score_match = re.search(r'<score>(.*?)</score>', prediction, re.DOTALL)\n",
        "                    pred_score_text = score_match.group(1).strip() if score_match else None\n",
        "\n",
        "                    score_match = re.search(r'<score>(.*?)</score>', expected, re.DOTALL)\n",
        "                    expected_score_text = score_match.group(1).strip() if score_match else None\n",
        "\n",
        "                # Method 3: Look for \"Rating:\" pattern\n",
        "                elif \"Rating:\" in prediction:\n",
        "                    import re\n",
        "                    rating_match = re.search(r'Rating:\\s*(\\d+)', prediction)\n",
        "                    pred_score_text = rating_match.group(1) if rating_match else None\n",
        "\n",
        "                    rating_match = re.search(r'Rating:\\s*(\\d+)', expected)\n",
        "                    expected_score_text = rating_match.group(1) if rating_match else None\n",
        "\n",
        "                # Convert to float if possible\n",
        "                pred_score = float(pred_score_text) if pred_score_text else None\n",
        "                expected_score = float(expected_score_text) if expected_score_text else None\n",
        "\n",
        "                if pred_score is not None and expected_score is not None:\n",
        "                    score_diff = abs(pred_score - expected_score)\n",
        "                    results.append({\n",
        "                        \"expected_score\": expected_score,\n",
        "                        \"predicted_score\": pred_score,\n",
        "                        \"score_difference\": score_diff\n",
        "                    })\n",
        "                    print(f\"Expected score: {expected_score}, Predicted score: {pred_score}, Difference: {score_diff}\")\n",
        "                else:\n",
        "                    print(\"Could not extract valid scores from prediction or expected output\")\n",
        "                    print(f\"Extracted pred_score_text: {pred_score_text}\")\n",
        "                    print(f\"Extracted expected_score_text: {expected_score_text}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting scores: {e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during generation: {e}\")\n",
        "\n",
        "    # Calculate mean absolute error\n",
        "    if results:\n",
        "        mae = sum(r[\"score_difference\"] for r in results) / len(results)\n",
        "        print(f\"\\nMean Absolute Error on test samples: {mae:.2f}\")\n",
        "    else:\n",
        "        print(\"\\nNo valid results to calculate Mean Absolute Error\")\n",
        "\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "B8azS-34Imx0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution flow\n",
        "dataset = load_or_generate_dataset()\n",
        "\n",
        "        # Format with tags\n",
        "formatted_data = format_with_tags(dataset)\n",
        "\n",
        "        # Split dataset\n",
        "train_data, val_data, test_data = split_dataset(formatted_data)\n",
        "\n",
        "        # Save test data for later evaluation\n",
        "with open(f\"{BASE_PATH}/test_data.json\", 'w') as f:\n",
        "          json.dump(test_data, f)\n",
        "\n",
        "        # Create HF datasets\n",
        "train_dataset, val_dataset = create_hf_datasets(train_data, val_data)\n",
        "\n",
        "        # Train model\n",
        "model_path = setup_and_train_model(train_dataset, val_dataset)\n",
        "\n",
        "        # Test model\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "PAKovAQdIBVL",
        "outputId": "03000595-3856-465d-96b6-80577dca99cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_or_generate_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2597516480.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Main execution flow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_or_generate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# Format with tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mformatted_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_with_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_or_generate_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #"
      ],
      "metadata": {
        "id": "5EHgv_j8TKfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path to your trained model\n",
        "# model_path = \"/content/drive/MyDrive/resume_skill_scoring/fine-tuned-model\"  # Update this path/\n",
        "BASE_PATH = \"/content/drive/MyDrive/resume_skill_scoring\"\n",
        "model_path = f\"{BASE_PATH}/fine-tuned-model_Phi-3\"\n",
        "base_model = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "def load_model_method1():\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        load_in_4bit=True,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_model_method1()"
      ],
      "metadata": {
        "id": "D-CBLvXcSsyj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222,
          "referenced_widgets": [
            "f97ff4a9445f470c86690bed37b922b9",
            "692c897a2adb476b9dc7acc095512a19",
            "018601b6ef154dfc9fd567a7a41c2f9f",
            "2882bd18faf34f4ea97a48db2e6be59f",
            "abee715a09bb4a7984d01f6d84f91ee1",
            "adf18374dd8f46e9ad6053966e8e4e2b",
            "b435a352893b482084e3bcdd1c71f6c7",
            "b52dc6cc72d7449fbb9234fca27a2a24",
            "63b588fd7fe64fedac9d851f5fe4f6e2",
            "29a09a4d30804df2a9cdb767a11eb4ee",
            "9625be00d2364a7e8a6db4d4fe7f1b04"
          ]
        },
        "outputId": "55846d2e-f8ac-4bbe-c046-d71ad3bbbcfe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f97ff4a9445f470c86690bed37b922b9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this in a cell to update transformers\n",
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezNhllTV8S30",
        "outputId": "85e38213-4d79-4d09-dd38-14bba4c12c1b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import re\n",
        "import torch\n",
        "\n",
        "def fast_test_single(model, tokenizer, resume, skill):\n",
        "    \"\"\"Fast single skill test with multiple prompt formats.\"\"\"\n",
        "\n",
        "    # Try 3 different formats to see which works best with your model\n",
        "    formats = {\n",
        "        'original': f\"\"\"<instruction>Evaluate the level of expertise for a specific skill in a resume.</instruction>\n",
        "<resume>{resume[:1000]}</resume>\n",
        "<skill>{skill}</skill>\n",
        "<rating_scale>\n",
        "0: Not mentioned\n",
        "1: Mentioned but no evidence of usage\n",
        "2: Basic usage demonstrated\n",
        "3: Moderate competency shown\n",
        "4: Strong competency with specific achievements\n",
        "5: Expert level with leadership/teaching in that skill\n",
        "</rating_scale>\n",
        "<answer>\"\"\",\n",
        "\n",
        "        'simplified': f\"\"\"<instruction>Evaluate {skill} skill level (0-5).</instruction>\n",
        "<resume>{resume[:1000]}</resume>\n",
        "<skill>{skill}</skill>\n",
        "<answer>\"\"\",\n",
        "\n",
        "        'ultra_simple': f\"Rate {skill} skill (0-5) in this resume:\\n{resume[:800]}\\n\\nScore:\"\n",
        "    }\n",
        "\n",
        "    best_score = None\n",
        "    best_response = \"\"\n",
        "    best_format = \"\"\n",
        "\n",
        "    for format_name, prompt in formats.items():\n",
        "        try:\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    max_new_tokens=80,\n",
        "                    temperature=0.0,\n",
        "                    do_sample=False,\n",
        "                    use_cache=False,\n",
        "                    early_stopping=True,\n",
        "                )\n",
        "\n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            generated = response[len(prompt):].strip()\n",
        "\n",
        "            # Extract score\n",
        "            score = None\n",
        "            for pattern in [r'<rating>(\\d+)', r'(\\d+)', r'Score:\\s*(\\d+)']:\n",
        "                match = re.search(pattern, generated)\n",
        "                if match:\n",
        "                    try:\n",
        "                        score = int(match.group(1))\n",
        "                        if 0 <= score <= 5:  # Valid score range\n",
        "                            break\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "            if score is not None:\n",
        "                best_score = score\n",
        "                best_response = generated\n",
        "                best_format = format_name\n",
        "                break  # Use first format that works\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    return best_score, best_response, best_format\n",
        "\n",
        "def fast_test_multiple(model, tokenizer, resume, skills_list):\n",
        "    \"\"\"Test multiple skills efficiently.\"\"\"\n",
        "\n",
        "    print(\"🚀 FAST TESTING MODE\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    results = {}\n",
        "    total_start = time.time()\n",
        "\n",
        "    for i, skill in enumerate(skills_list):\n",
        "        print(f\"\\n[{i+1}/{len(skills_list)}] Testing {skill}...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        score, response, format_used = fast_test_single(model, tokenizer, resume, skill)\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        results[skill] = {\n",
        "            'score': score,\n",
        "            'response': response[:150] + \"...\" if len(response) > 150 else response,\n",
        "            'format': format_used,\n",
        "            'time': elapsed\n",
        "        }\n",
        "\n",
        "        print(f\"  Score: {score} | Format: {format_used} | Time: {elapsed:.1f}s\")\n",
        "        if score is None:\n",
        "            print(f\"  ⚠️  Could not extract valid score\")\n",
        "\n",
        "    total_time = time.time() - total_start\n",
        "\n",
        "    print(f\"\\n⏱️ Total time: {total_time:.1f}s\")\n",
        "    print(\"\\n📊 FINAL RESULTS:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for skill, result in results.items():\n",
        "        score_display = result['score'] if result['score'] is not None else \"N/A\"\n",
        "        print(f\"{skill:15}: {score_display}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def quick_diagnostic(model, tokenizer):\n",
        "    \"\"\"Quick diagnostic to see what your model learned.\"\"\"\n",
        "\n",
        "    print(\"🔧 DIAGNOSTIC TEST\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    diagnostic_cases = [\n",
        "        (\"Expert Python developer with 5+ years\", \"Python\", \"Expected: 4-5\"),\n",
        "        (\"No programming mentioned\", \"Python\", \"Expected: 0\"),\n",
        "        (\"Built ML models with PyTorch\", \"Machine Learning\", \"Expected: 3-4\"),\n",
        "        (\"Led team of 5 engineers\", \"Leadership\", \"Expected: 3-4\"),\n",
        "        (\"No Java mentioned anywhere\", \"Java\", \"Expected: 0\"),\n",
        "    ]\n",
        "\n",
        "    for resume_snippet, skill, expected in diagnostic_cases:\n",
        "        score, response, format_used = fast_test_single(model, tokenizer, resume_snippet, skill)\n",
        "        print(f\"\\nResume: {resume_snippet}\")\n",
        "        print(f\"Skill: {skill}\")\n",
        "        print(f\"Got: {score} | {expected} | Format: {format_used}\")\n",
        "        print(f\"Full Response: {response}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Your test resume\n",
        "sample_resume = \"\"\"\n",
        "ALEX MORGAN\n",
        "alex.morgan@email.com | (555) 123-4567 | linkedin.com/in/alexmorgan\n",
        "\n",
        "SUMMARY\n",
        "Recent computer science graduate with specialized focus on generative AI models. Experience implementing and fine-tuning various generative architectures including GANs, VAEs, and diffusion models through academic projects and internships.\n",
        "\n",
        "WORK EXPERIENCE\n",
        "\n",
        "Junior Machine Learning Engineer (Internship)\n",
        "TechnoVision AI | May 2023 - August 2023\n",
        "• Developed and fine-tuned a conditional GAN model that generated synthetic medical images with 85% improved quality\n",
        "• Implemented a personalized text-to-image diffusion model that incorporated user preferences\n",
        "• Collaborated with a team of 5 engineers to create documentation and training materials\n",
        "• Assisted in evaluating and selecting generative model architectures based on performance metrics\n",
        "\n",
        "AI Research Assistant\n",
        "University Research Lab | September 2022 - May 2023\n",
        "• Conducted research on variational autoencoders (VAEs) for 3D shape generation\n",
        "• Implemented improvements to existing diffusion models that reduced training time by 30%\n",
        "• Contributed to a paper on ethical considerations in generative AI\n",
        "• Maintained and updated lab's computing infrastructure for efficient training of large generative models\n",
        "\n",
        "EDUCATION\n",
        "Master of Science in Computer Science (Specialization in Machine Learning)\n",
        "Tech University | 2022 - 2023\n",
        "\n",
        "SKILLS\n",
        "• Languages: Python, C++, JavaScript, SQL\n",
        "• Frameworks: PyTorch, TensorFlow, Keras, Hugging Face Transformers\n",
        "• Generative Models: GANs, VAEs, Diffusion Models, Transformers\n",
        "• Cloud Platforms: AWS (SageMaker, EC2), Google Cloud Platform\n",
        "\"\"\"\n",
        "\n",
        "# ============= RUN TESTS =============\n",
        "\n",
        "# 1. Quick diagnostic first\n",
        "quick_diagnostic(model, tokenizer)\n",
        "\n",
        "# 2. Test multiple skills on the sample resume\n",
        "skills_to_test = [\n",
        "    \"Python\",           # Should be 4-5\n",
        "    \"Machine Learning\", # Should be 4-5\n",
        "    \"PyTorch\",         # Should be 4\n",
        "    \"Leadership\",      # Should be 2-3\n",
        "    \"Java\",           # Should be 0\n",
        "    \"Communication\",  # Should be 2-3\n",
        "]\n",
        "\n",
        "results = fast_test_multiple(model, tokenizer, sample_resume, skills_to_test)\n",
        "\n",
        "# 3. Analysis\n",
        "print(\"\\n🔍 ANALYSIS:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "correct_scores = 0\n",
        "total_scores = 0\n",
        "\n",
        "expected_ranges = {\n",
        "    \"Python\": (4, 5),\n",
        "    \"Machine Learning\": (4, 5),\n",
        "    \"PyTorch\": (3, 4),\n",
        "    \"Leadership\": (2, 3),\n",
        "    \"Java\": (0, 1),\n",
        "    \"Communication\": (2, 3)\n",
        "}\n",
        "\n",
        "for skill, (min_exp, max_exp) in expected_ranges.items():\n",
        "    if skill in results and results[skill]['score'] is not None:\n",
        "        actual = results[skill]['score']\n",
        "        total_scores += 1\n",
        "        if min_exp <= actual <= max_exp:\n",
        "            correct_scores += 1\n",
        "            status = \"✅\"\n",
        "        else:\n",
        "            status = \"❌\"\n",
        "        print(f\"{status} {skill}: Got {actual}, Expected {min_exp}-{max_exp}\")\n",
        "\n",
        "if total_scores > 0:\n",
        "    accuracy = correct_scores / total_scores * 100\n",
        "    print(f\"\\nAccuracy: {correct_scores}/{total_scores} ({accuracy:.1f}%)\")\n",
        "\n",
        "    if accuracy < 50:\n",
        "        print(\"\\n⚠️  LOW ACCURACY - Your model needs retraining!\")\n",
        "        print(\"Issues likely in training data or model configuration.\")\n",
        "    elif accuracy < 80:\n",
        "        print(\"\\n🟡 MODERATE ACCURACY - Consider DPO for improvement\")\n",
        "    else:\n",
        "        print(\"\\n✅ GOOD ACCURACY - Model working well!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqK0yE8d6in2",
        "outputId": "d61ad572-9fb8-4ce0-fb83-0ca49fdc38fa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 DIAGNOSTIC TEST\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resume: Expert Python developer with 5+ years\n",
            "Skill: Python\n",
            "Got: 3 | Expected: 4-5 | Format: original\n",
            "Full Response: 3>\n",
            "<evidence_rating>weak</evidence_rating>\n",
            "<justification>The resume demonstrates some Python usage but lacks specific achievements or projects showcasing leadership/teaching in Python. The resume mentions Python but doesn't provide evidence of mentoring or teaching others.</justification>\n",
            "</answer>\n",
            "---\n",
            "\n",
            "<resume>\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resume: No programming mentioned\n",
            "Skill: Python\n",
            "Got: 0 | Expected: 0 | Format: original\n",
            "Full Response: 0</answer>\n",
            "\n",
            "\n",
            "resume_text:\n",
            "# MARTIN JOHNSON\n",
            "\n",
            "123 Coding Lane, Tech City, CA 90210\n",
            "(415) 555-0178 | mjohnson@email.com | linkedin.com/in/martinjohn\n",
            "\n",
            "##\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resume: Built ML models with PyTorch\n",
            "Skill: Machine Learning\n",
            "Got: 3 | Expected: 3-4 | Format: original\n",
            "Full Response: 3>Moderate competency shown</answer>\n",
            "<thought>The resume shows some ML experience but lacks depth. I need to assess the ML skills more thoroughly.</thought>\n",
            "\n",
            "<resume>Data Scientist with 3+ years of experience in ML and Python</resume>\n",
            "<skill>Machine Learning</skill>imetrics</rating\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resume: Led team of 5 engineers\n",
            "Skill: Leadership\n",
            "Got: 3 | Expected: 3-4 | Format: original\n",
            "Full Response: 3</rating_scale>\n",
            "<justification>The resume demonstrates leadership experience by mentioning a team of 5 engineers, indicating the candidate has led a group of professionals. However, there is no evidence of specific leadership projects, initiatives, or achievements. The resume shows moderate competency in leadership as it demonstrates some experience in managing a team but\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resume: No Java mentioned anywhere\n",
            "Skill: Java\n",
            "Got: 0 | Expected: 0 | Format: original\n",
            "Full Response: 0</answer>\n",
            "\n",
            "---\n",
            "\n",
            "<question>\n",
            "A resume for a Senior Data Scientist position in a tech company.\n",
            "\n",
            "resume_text:\n",
            "\n",
            "Alexandra Petrova\n",
            "senior-level Data Scientist with 12+ years of experience in machine learning, data engineering, and business analytics. Specializes in developing predictive models and data\n",
            "----------------------------------------\n",
            "🚀 FAST TESTING MODE\n",
            "==================================================\n",
            "\n",
            "[1/6] Testing Python...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Score: 4 | Format: original | Time: 19.6s\n",
            "\n",
            "[2/6] Testing Machine Learning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Score: 4 | Format: original | Time: 19.6s\n",
            "\n",
            "[3/6] Testing PyTorch...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Score: 3 | Format: original | Time: 19.6s\n",
            "\n",
            "[4/6] Testing Leadership...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Score: 2 | Format: original | Time: 19.6s\n",
            "\n",
            "[5/6] Testing Java...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Score: 2 | Format: original | Time: 19.5s\n",
            "\n",
            "[6/6] Testing Communication...\n",
            "  Score: 1 | Format: original | Time: 19.6s\n",
            "\n",
            "⏱️ Total time: 117.5s\n",
            "\n",
            "📊 FINAL RESULTS:\n",
            "----------------------------------------\n",
            "Python         : 4\n",
            "Machine Learning: 4\n",
            "PyTorch        : 3\n",
            "Leadership     : 2\n",
            "Java           : 2\n",
            "Communication  : 1\n",
            "\n",
            "🔍 ANALYSIS:\n",
            "------------------------------\n",
            "✅ Python: Got 4, Expected 4-5\n",
            "✅ Machine Learning: Got 4, Expected 4-5\n",
            "✅ PyTorch: Got 3, Expected 3-4\n",
            "✅ Leadership: Got 2, Expected 2-3\n",
            "❌ Java: Got 2, Expected 0-1\n",
            "❌ Communication: Got 1, Expected 2-3\n",
            "\n",
            "Accuracy: 4/6 (66.7%)\n",
            "\n",
            "🟡 MODERATE ACCURACY - Consider DPO for improvement\n"
          ]
        }
      ]
    }
  ]
}