{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJuykrnmDE3U7vWWyDtT3K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zodbot/llm_finetuning/blob/main/lora_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "log1FjOVFoYu",
        "outputId": "f6f83675-94be-455c-c42e-6b76ec775892"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt_download.py', <http.client.HTTPMessage at 0x7f6f50c50890>)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import urllib.request\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/\"\n",
        "    \"LLMs-from-scratch/main/ch05/\"\n",
        "    \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "filename = url.split('/')[-1]\n",
        "urllib.request.urlretrieve(url, filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from gpt_download import download_and_load_gpt2\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=\"124M\",\n",
        "    models_dir=\"/content/drive/MyDrive/gpt2\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AmPJN9NFw3N",
        "outputId": "37587247-c2af-4d2f-e6a9-477efdbcd31c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "File already exists and is up-to-date: /content/drive/MyDrive/gpt2/124M/checkpoint\n",
            "File already exists and is up-to-date: /content/drive/MyDrive/gpt2/124M/encoder.json\n",
            "File already exists and is up-to-date: /content/drive/MyDrive/gpt2/124M/hparams.json\n",
            "File already exists and is up-to-date: /content/drive/MyDrive/gpt2/124M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: /content/drive/MyDrive/gpt2/124M/model.ckpt.index\n",
            "File already exists and is up-to-date: /content/drive/MyDrive/gpt2/124M/model.ckpt.meta\n",
            "File already exists and is up-to-date: /content/drive/MyDrive/gpt2/124M/vocab.bpe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "\n",
        "\n",
        "!git clone https://github.com/zodbot/llm_finetuning.git\n",
        "\n",
        "# Change into repo directory\n",
        "%cd llm_finetuning\n",
        "\n",
        "from src.model import GPTModel\n",
        "from src.config import GPT_CONFIGS\n",
        "from src.utils import load_weights_into_gpt\n",
        "\n",
        "# Get configuration\n",
        "config = GPT_CONFIGS[\"gpt2-small (124M)\"]\n",
        "\n",
        "# Set up model for classification\n",
        "model = GPTModel(config)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval()\n",
        "\n",
        "# Freeze model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify for classification\n",
        "num_classes = 2\n",
        "model.out_head = torch.nn.Linear(config[\"emb_dim\"], num_classes)\n",
        "\n",
        "# Unfreeze specific layers\n",
        "for param in model.trf_blocks[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model.final_norm.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "dQ2ly4IoFy2L",
        "outputId": "93f1e3cc-df5a-440a-9868-24d360883268"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llm_finetuning'...\n",
            "remote: Enumerating objects: 121, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (102/102), done.\u001b[K\n",
            "remote: Total 121 (delta 50), reused 73 (delta 16), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (121/121), 1.27 MiB | 13.14 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n",
            "/content/llm_finetuning\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-610efdf60657>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Modify for classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"emb_dim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Unfreeze specific layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SST-2 (Stanford Sentiment Treebank) is a great dataset for binary sentiment classification\n",
        "from datasets import load_dataset\n",
        "ds = load_dataset('nyu-mll/glue', 'sst2', split='train')"
      ],
      "metadata": {
        "id": "vgsTzh59F0_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to pandas DataFrame\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "    'Label': ds['label'],\n",
        "    'Text': ds['sentence']\n",
        "})\n",
        "\n",
        "print(len(df))\n",
        "print(df[\"Label\"].value_counts())\n",
        "\n",
        "# Take random sample of 1000 rows\n",
        "df_sample = df.sample(n=10000, random_state=42)  # random_state for reproducibility\n",
        "\n",
        "# Optional: Look at the distribution of labels to ensure it's balanced\n",
        "print(\"Label distribution in sample:\")\n",
        "print(df_sample['Label'].value_counts())\n",
        "\n",
        "def random_split(df, train_frac, validation_frac):\n",
        "    df = df.sample(\n",
        "        frac=1, random_state=123\n",
        "    ).reset_index(drop=True)\n",
        "    train_end = int(len(df) * train_frac)\n",
        "    validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "    train_df = df[:train_end]\n",
        "    validation_df = df[train_end:validation_end]\n",
        "    test_df = df[validation_end:]\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "train_df, validation_df, test_df = random_split(df, 0.7, 0.1)\n",
        "\n",
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)\n",
        "\n",
        "f = open(\"train.csv\")\n",
        "inputs = []\n",
        "max_length = 0\n",
        "for line in f.read():\n",
        "  ids = tokenizer.encode(line)\n",
        "  max_length = max(max_length, len(ids))\n",
        "  inputs.append(ids)\n",
        "\n",
        "\n",
        "for input in inputs:\n",
        "  for _ in range(max_length - len(input)):\n",
        "    input.append(5027)"
      ],
      "metadata": {
        "id": "cKQoAJV9F3-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4_mVXamiGD9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "# it identifies the longest sequence in the training dataset, encodes the text messages,\n",
        "# and ensures that all other sequences are padded with a padding token to match the length of the longest sequence.\n",
        "class Sst2DataSet(Dataset):\n",
        "  def __init__(self, csv_file, tokenizer, max_length=None,\n",
        "                 pad_token_id=50256):\n",
        "      self.data = pd.read_csv(csv_file)\n",
        "      self.encoded_texts = [tokenizer.encode(data) for data in self.data[\"Text\"]]\n",
        "      if max_length is None:\n",
        "          self.max_length = self._longest_length()\n",
        "      else:\n",
        "          self.max_length = max_length\n",
        "      # Truncates sequences if they are longer than max_length\n",
        "      self.encoded_texts = [\n",
        "                  encoded_text[:self.max_length]\n",
        "                  for encoded_text in self.encoded_texts\n",
        "      ]\n",
        "      # add padding\n",
        "      self.encoded_texts = [\n",
        "        encoded_text + [pad_token_id] *\n",
        "        (self.max_length - len(encoded_text))\n",
        "        for encoded_text in self.encoded_texts\n",
        "      ]\n",
        "\n",
        " def __getitem__(self, index):\n",
        "      encoded = self.encoded_texts[index]\n",
        "      label = self.data.iloc[index][\"Label\"]\n",
        "      return (\n",
        "          torch.tensor(encoded, dtype=torch.long),\n",
        "          torch.tensor(label, dtype=torch.long)\n",
        "      )\n",
        "  def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "  def _longest_length(self):\n",
        "      max_length = 0\n",
        "      for encoded_text in self.encoded_texts:\n",
        "          encoded_length = len(encoded_text)\n",
        "          if encoded_length > max_length:\n",
        "              max_length = encoded_length\n",
        "      return max_length\n",
        "\n",
        "\n",
        "train_dataset = Sst2DataSet(\n",
        "    csv_file=\"train.csv\",\n",
        "    max_length=None,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "val_dataset = Sst2DataSet(\n",
        "    csv_file=\"validation.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "test_dataset = Sst2DataSet(\n",
        "    csv_file=\"test.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "print(train_dataset.max_length)"
      ],
      "metadata": {
        "id": "d5QCv3cWGJNO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}